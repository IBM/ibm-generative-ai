"""Wrapper around IBM GENAI APIs for use in Langchain"""
import asyncio
import logging
from functools import partial
from pathlib import Path
from typing import Any, Iterator, List, Mapping, Optional, Union

from pydantic import ConfigDict

from genai.exceptions import GenAiException
from genai.utils.general import to_model_instance

try:
    from langchain.callbacks.manager import (
        AsyncCallbackManagerForLLMRun,
        CallbackManagerForLLMRun,
    )
    from langchain.llms.base import LLM
    from langchain.schema import LLMResult
    from langchain.schema.output import GenerationChunk

    from .utils import (
        create_generation_info_from_result,
        create_llm_output,
        load_config,
        update_llm_result,
        update_token_usage,
    )
except ImportError:
    raise ImportError("Could not import langchain: Please install ibm-generative-ai[langchain] extension.")

from genai import Credentials, Model
from genai.schemas import GenerateParams

logger = logging.getLogger(__name__)

__all__ = ["LangChainInterface"]


class LangChainInterface(LLM):
    """
    Wrapper around IBM GENAI models.
    To use, you should have the ``genai`` python package installed
    and initialize the credentials attribute of this class with
    an instance of ``genai.Credentials``. Model specific parameters
    can be passed through to the constructor using the ``params``
    parameter, which is an instance of GenerateParams.
    Example:
        .. code-block:: python
            llm = LangChainInterface(model="google/flan-ul2", credentials=creds)
    """

    credentials: Credentials
    model: str
    params: Optional[GenerateParams] = None
    model_config = ConfigDict(extra="forbid", protected_namespaces=())

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        _params = to_model_instance(self.params, GenerateParams)
        return {"model": self.model, "params": _params.model_dump()}

    @classmethod
    def load_from_file(cls, file: Union[str, Path], *, credentials: Credentials):
        config = load_config(file)
        return cls(**config, credentials=credentials)

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "IBM GENAI"

    @classmethod
    def is_lc_serializable(cls) -> bool:
        return True

    @property
    def lc_secrets(self) -> dict[str, str]:
        return {"credentials": "CREDENTIALS"}

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Call the IBM GENAI's inference endpoint.
        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
            run_manager: Optional callback manager.
        Returns:
            The string generated by the model.
        Example:
            .. code-block:: python
                llm = LangChainInterface(model_id="google/flan-ul2", credentials=creds)
                response = llm("What is a molecule")
        """
        result = self._generate(prompts=[prompt], stop=stop, run_manager=run_manager, **kwargs)
        return result.generations[0][0].text

    def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> LLMResult:
        final_result = LLMResult(generations=[], llm_output=create_llm_output(model=self.model))
        assert final_result.llm_output

        if len(prompts) == 0:
            return final_result

        params = to_model_instance(self.params, GenerateParams)
        params.stop_sequences = stop or params.stop_sequences

        if params.stream:
            if len(prompts) != 1:
                raise GenAiException(ValueError("Streaming works only for a single prompt."))

            generation = GenerationChunk(text="", generation_info={})
            for result in self._stream(
                prompt=prompts[0],
                stop=params.stop_sequences,
                run_manager=run_manager,
                **kwargs,
            ):
                chunk = GenerationChunk(text=result.text)
                if result.generation_info:
                    update_token_usage(
                        target=final_result.llm_output, sources=[result.generation_info.get("token_usage")]
                    )
                    chunk.generation_info = result.generation_info.copy()
                generation += chunk

            final_result.generations.append([generation])
            return final_result

        model = Model(model=self.model, params=params, credentials=self.credentials)
        for response in model.generate(
            prompts=prompts,
            **kwargs,
        ):
            chunk = GenerationChunk(
                text=response.generated_text or "",
            )
            logger.info("Output of GENAI call: {}".format(chunk.text))
            chunk.generation_info = create_generation_info_from_result(response)
            update_llm_result(current=final_result, generation_info=chunk.generation_info)
            final_result.generations.append([chunk])

        return final_result

    async def _agenerate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> LLMResult:
        return await asyncio.get_running_loop().run_in_executor(
            None, partial(self._generate, prompts, stop, run_manager, **kwargs)
        )

    def _stream(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[GenerationChunk]:
        """Call the IBM GENAI's inference endpoint which then streams the response.
        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
            run_manager: Optional callback manager.
        Returns:
            The iterator which yields generation chunks.
        Example:
            .. code-block:: python
                llm = LangChainInterface(model_id="google/flan-ul2", credentials=creds)
                for chunk in llm.stream("What is a molecule?"):
                    print(chunk.text)
        """
        params = to_model_instance(self.params, GenerateParams)
        params.stop_sequences = stop or params.stop_sequences

        model = Model(model=self.model, params=params, credentials=self.credentials)
        for result in model.generate_stream(prompts=[prompt], **kwargs):
            logger.info("Chunk received: {}".format(result.generated_text))
            chunk = GenerationChunk(
                text=result.generated_text or "",
                generation_info=create_generation_info_from_result(result),
            )
            yield chunk
            if run_manager:
                run_manager.on_llm_new_token(token=chunk.text, chunk=chunk, response=result)

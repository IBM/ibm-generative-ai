"""Wrapper around IBM GENAI APIs for use in Langchain"""
import asyncio
import logging
from functools import partial
from pathlib import Path
from typing import Any, Iterator, List, Mapping, Optional, Union

from pydantic import ConfigDict

from genai.exceptions import GenAiException
from genai.utils.general import to_model_instance

try:
    from langchain.callbacks.manager import (
        AsyncCallbackManagerForLLMRun,
        CallbackManagerForLLMRun,
    )
    from langchain.llms.base import LLM
    from langchain.schema import BaseMessage, LLMResult, get_buffer_string
    from langchain.schema.output import GenerationChunk

    from .utils import (
        create_generation_info,
        create_generation_info_from_response,
        create_llm_output,
        load_config,
        update_token_usage,
        update_token_usage_stream,
    )
except ImportError:
    raise ImportError("Could not import langchain: Please install ibm-generative-ai[langchain] extension.")

from genai import Credentials, Model
from genai.schemas import GenerateParams

logger = logging.getLogger(__name__)

__all__ = ["LangChainInterface"]


class LangChainInterface(LLM):
    """
    Wrapper around IBM GENAI models.
    To use, you should have the ``genai`` python package installed
    and initialize the credentials attribute of this class with
    an instance of ``genai.Credentials``. Model specific parameters
    can be passed through to the constructor using the ``params``
    parameter, which is an instance of GenerateParams.
    Example:
        .. code-block:: python
            llm = LangChainInterface(model="google/flan-ul2", credentials=creds)
    """

    credentials: Credentials
    model: str
    params: Optional[GenerateParams] = None
    model_config = ConfigDict(extra="forbid", protected_namespaces=())
    streaming: Optional[bool] = None

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        _params = to_model_instance(self.params, GenerateParams)
        return {"model": self.model, "params": _params.model_dump()}

    @classmethod
    def load_from_file(cls, file: Union[str, Path], *, credentials: Credentials):
        config = load_config(file)
        return cls(**config, credentials=credentials)

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "IBM GENAI"

    @classmethod
    def is_lc_serializable(cls) -> bool:
        return True

    @property
    def lc_secrets(self) -> dict[str, str]:
        return {"credentials": "CREDENTIALS"}

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Call the IBM GENAI's inference endpoint.
        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
            run_manager: Optional callback manager.
        Returns:
            The string generated by the model.
        Example:
            .. code-block:: python
                llm = LangChainInterface(model_id="google/flan-ul2", credentials=creds)
                response = llm("What is a molecule")
        """
        result = self._generate(prompts=[prompt], stop=stop, run_manager=run_manager, **kwargs)
        return result.generations[0][0].text

    def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> LLMResult:
        final_result = LLMResult(generations=[], llm_output=create_llm_output(model=self.model))
        assert final_result.llm_output

        if len(prompts) == 0:
            return final_result

        params = to_model_instance(self.params, GenerateParams)
        params.stop_sequences = stop or params.stop_sequences
        params.stream = params.stream or self.streaming

        if params.stream:
            if len(prompts) != 1:
                raise GenAiException(ValueError("Streaming works only for a single prompt."))

            generation = GenerationChunk(text="", generation_info=create_generation_info())

            for result in self._stream(
                prompt=prompts[0],
                stop=params.stop_sequences,
                run_manager=run_manager,
                **kwargs,
            ):
                token_usage = result.generation_info.pop("token_usage")
                generation += result
                update_token_usage_stream(
                    target=generation.generation_info["token_usage"],
                    source=token_usage,
                )

            final_result.generations.append([generation])
            update_token_usage(
                target=final_result.llm_output["token_usage"], source=generation.generation_info["token_usage"]
            )

            return final_result

        model = Model(model=self.model, params=params, credentials=self.credentials)
        for response in model.generate_as_completed(
            prompts=prompts,
            **kwargs,
            raw_response=True,
        ):
            for result in response.results:
                generation_info = create_generation_info_from_response(response, result=result)

                chunk = GenerationChunk(
                    text=result.generated_text or "",
                    generation_info=generation_info,
                )
                logger.info("Output of GENAI call: {}".format(chunk.text))
                update_token_usage(target=final_result.llm_output["token_usage"], source=generation_info["token_usage"])
                final_result.generations.append([chunk])

        return final_result

    async def _agenerate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> LLMResult:
        return await asyncio.get_running_loop().run_in_executor(
            None, partial(self._generate, prompts, stop, run_manager, **kwargs)
        )

    def _stream(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[GenerationChunk]:
        """Call the IBM GENAI's inference endpoint which then streams the response.
        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
            run_manager: Optional callback manager.
        Returns:
            The iterator which yields generation chunks.
        Example:
            .. code-block:: python
                llm = LangChainInterface(model_id="google/flan-ul2", credentials=creds)
                for chunk in llm.stream("What is a molecule?"):
                    print(chunk.text)
        """
        params = to_model_instance(self.params, GenerateParams)
        params.stop_sequences = stop or params.stop_sequences

        model = Model(model=self.model, params=params, credentials=self.credentials)
        for response in model.generate_stream(prompts=[prompt], raw_response=True, **kwargs):

            def send_chunk(*, text: Optional[str] = None, generation_info: dict):
                logger.info("Chunk received: {}".format(text))
                chunk = GenerationChunk(
                    text=text or "",
                    generation_info=generation_info.copy(),
                )
                yield chunk
                if run_manager:
                    run_manager.on_llm_new_token(token=chunk.text, chunk=chunk, response=response)

            if response.moderation:
                generation_info = create_generation_info_from_response(response, result=response.moderation)
                yield from send_chunk(generation_info=generation_info)

            for result in response.results or []:
                generation_info = create_generation_info_from_response(response, result=result)
                yield from send_chunk(text=result.generated_text, generation_info=generation_info)

    def get_num_tokens(self, text: str) -> int:
        model = Model(self.model, params=self.params, credentials=self.credentials)
        response = model.tokenize([text], return_tokens=False)[0]
        assert response.token_count is not None
        return response.token_count

    def get_num_tokens_from_messages(self, messages: list[BaseMessage]) -> int:
        model = Model(self.model, params=self.params, credentials=self.credentials)
        responses = model.tokenize([get_buffer_string([message]) for message in messages], return_tokens=False)
        return sum([response.token_count for response in responses if response.token_count])

    def get_token_ids(self, text: str) -> List[int]:
        raise NotImplementedError("API does not support returning token ids.")

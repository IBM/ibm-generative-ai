# generated by datamodel-codegen:
#   filename:  2024-01-30_openapi_schema

from __future__ import annotations

from datetime import date
from enum import Enum
from typing import Any, Literal, Optional, Union

from pydantic import AwareDatetime, Field, RootModel

from genai._types import ApiBaseModel


class ApiKeyResult(ApiBaseModel):
    created_at: AwareDatetime
    generated_at: AwareDatetime
    last_used_at: Optional[AwareDatetime] = None
    value: str


class BaseErrorExtension(ApiBaseModel):
    code: str
    state: Optional[dict[str, Any]] = None


class BaseErrorResponse(ApiBaseModel):
    error: str
    extensions: BaseErrorExtension
    message: str
    status_code: int


class ChatRole(str, Enum):
    USER = "user"
    SYSTEM = "system"
    ASSISTANT = "assistant"


class ConcurrencyLimit(ApiBaseModel):
    limit: int
    remaining: int


class DecodingMethod(str, Enum):
    GREEDY = "greedy"
    SAMPLE = "sample"


class FileFormat(ApiBaseModel):
    id: int
    name: str


class FileListSortBy(str, Enum):
    NAME = "name"
    CREATED_AT = "created_at"


class FilePurpose(str, Enum):
    TUNE = "tune"
    TEMPLATE = "template"
    TUNE_IMPORT = "tune_import"
    EXTRACTION = "extraction"


class GeneratedToken(ApiBaseModel):
    logprob: Optional[Union[float, str]] = None
    text: Optional[str] = None


class HAPOptions(ApiBaseModel):
    send_tokens: Optional[bool] = False
    threshold: Optional[float] = Field(0.75, gt=0.0, lt=1.0)


class ImplicitHateOptions(ApiBaseModel):
    send_tokens: Optional[bool] = False
    threshold: Optional[float] = Field(0.75, gt=0.0, lt=1.0)


class Extensions1(BaseErrorExtension):
    code: Literal["INTERNAL_SERVER_ERROR"] = "INTERNAL_SERVER_ERROR"


class InternalServerErrorResponse(BaseErrorResponse):
    extensions: Extensions1
    status_code: Literal[500] = 500


class LengthPenalty(ApiBaseModel):
    decay_factor: Optional[float] = Field(None, gt=1.0, title="Decay factor")
    """
    Represents the factor of exponential decay and must be > 1.0. Larger values correspond to more aggressive decay.
    """
    start_index: Optional[int] = Field(None, ge=1, title="Start index")
    """
    A number of generated tokens after which this should take effect.
    """


class ModelFamily(ApiBaseModel):
    description: Optional[str] = None
    id: int
    name: str
    prompt_example: Optional[str] = None
    short_description: Optional[str] = None
    system_prompt: Optional[str] = None


class ModelTokenLimits(ApiBaseModel):
    beam_width: int
    token_limit: int


class ModerationHAP(ApiBaseModel):
    input: Optional[bool] = True
    """
    Detects HAP (hateful, abusive, or profane language). Please see documentation for more info (API Reference -> Moderations -> HAP).
    """
    output: Optional[bool] = None
    send_tokens: Optional[bool] = False
    threshold: Optional[float] = Field(0.75, ge=0.01, le=0.99, multiple_of=0.01)
    """
    The higher the number, the more confidence that the sentence contains HAP. The threshold allows you to modify how much confidence is needed for the sentence to be flagged as containing HAP.
    """


class ModerationImplicitHate(ApiBaseModel):
    input: Optional[bool] = True
    """
    Detects implicit hate.
    """
    output: Optional[bool] = None
    send_tokens: Optional[bool] = False
    threshold: Optional[float] = Field(0.75, ge=0.01, le=0.99, multiple_of=0.01)
    """
    The higher the number, the more confidence that the sentence contains implicit hate. The threshold allows you to modify how much confidence is needed for the sentence to be flagged as containing implicit hate.
    """


class ModerationPosition(ApiBaseModel):
    end: int
    start: int


class ModerationStigma(ApiBaseModel):
    input: Optional[bool] = True
    """
    Detects Stigma.
    """
    output: Optional[bool] = None
    send_tokens: Optional[bool] = False
    threshold: Optional[float] = Field(0.75, ge=0.01, le=0.99, multiple_of=0.01)
    """
    The higher the number, the more confidence that the sentence contains Stigma. The threshold allows you to modify how much confidence is needed for the sentence to be flagged as containing Stigma.
    """


class ModerationTokens(ApiBaseModel):
    index: Optional[int] = None
    score: Optional[float] = None
    token: Optional[str] = None


class Extensions2(BaseErrorExtension):
    code: Literal["NOT_FOUND"] = "NOT_FOUND"


class NotFoundResponse(BaseErrorResponse):
    extensions: Extensions2
    status_code: Literal[404] = 404


class PromptResultAuthor(ApiBaseModel):
    first_name: Optional[str] = None
    id: Optional[int] = None
    last_name: Optional[str] = None


class PromptResultTask(ApiBaseModel):
    icon: Optional[str] = None
    id: Optional[str] = None
    name: Optional[str] = None


class PromptRetrieveRequestParamsSource(str, Enum):
    USER = "user"
    EXAMPLE = "example"
    COMMUNITY = "community"


class PromptTemplateData(ApiBaseModel):
    example_file_ids: Optional[list[str]] = Field(None, max_length=5, min_length=0)


class PromptType(str, Enum):
    PRIVATE = "private"
    PUBLIC = "public"
    COMMUNITY = "community"
    EXAMPLE = "example"


class RequestApiVersion(str, Enum):
    V0 = "v0"
    V1 = "v1"
    V2 = "v2"


class RequestEndpoint(str, Enum):
    GENERATE = "generate"
    COMPARE = "compare"
    CHAT = "chat"


class RequestOrigin(str, Enum):
    API = "api"
    UI = "ui"


class RequestResultVersion(ApiBaseModel):
    api: Optional[str] = None
    date_: Optional[date] = Field(None, alias="date")


class RequestStatus(str, Enum):
    SUCCESS = "success"
    ERROR = "error"


class SortDirection(str, Enum):
    ASC = "asc"
    DESC = "desc"


class StigmaOptions(ApiBaseModel):
    send_tokens: Optional[bool] = False
    threshold: Optional[float] = Field(0.75, gt=0.0, lt=1.0)


class StopReason(str, Enum):
    NOT_FINISHED = "not_finished"
    MAX_TOKENS = "max_tokens"
    EOS_TOKEN = "eos_token"
    CANCELLED = "cancelled"
    TIME_LIMIT = "time_limit"
    STOP_SEQUENCE = "stop_sequence"
    TOKEN_LIMIT = "token_limit"
    ERROR = "error"


class StorageProviderLocation(str, Enum):
    US_SOUTH = "us-south"
    US_EAST = "us-east"


class SystemPromptAuthor(ApiBaseModel):
    first_name: Optional[str] = None
    id: int
    last_name: Optional[str] = None


class SystemPromptType(str, Enum):
    PRIVATE = "private"
    SYSTEM = "system"


class Tasks(ApiBaseModel):
    categorization: bool
    csv_example: Optional[str] = None
    file_format_id: Optional[int] = None
    id: str
    json_example: Optional[str] = None
    jsonl_example: Optional[str] = None
    name: str
    tune: bool
    verbalizer: Optional[str] = None


class TextEmbeddingLimit(ApiBaseModel):
    concurrency: ConcurrencyLimit


class TextEmbeddingParameters(ApiBaseModel):
    truncate_input_tokens: Optional[bool] = None


class TextGenerationComparisonCreateResultsParameters(ApiBaseModel):
    length_penalty: Optional[dict[str, Any]] = None
    model_id: Optional[str] = None
    repetition_penalty: Optional[float] = None
    temperature: Optional[float] = None
    top_k: Optional[int] = None
    top_p: Optional[float] = None
    typical_p: Optional[float] = None


class TextGenerationFeedbackCategory(str, Enum):
    NO_PROBLEM = "no_problem"
    HAP = "hap"
    PII = "pii"
    SOCIAL_BIAS = "social_bias"
    NOT_HONEST_OR_TRUTHFUL = "not_honest_or_truthful"
    TABOO_TOPICS = "taboo_topics"
    OTHER = "other"


class TextGenerationFeedbackResult(ApiBaseModel):
    api_request: str
    categories: list[str]
    comment: Optional[str] = None
    created_at: AwareDatetime
    id: int
    updated_at: AwareDatetime


class TextGenerationLimit(ApiBaseModel):
    concurrency: ConcurrencyLimit


class TextGenerationReturnOptions(ApiBaseModel):
    generated_tokens: Optional[bool] = Field(False, title="Generated Tokens")
    """
    Include list of individual generated tokens
    """
    input_parameters: Optional[bool] = None
    input_text: Optional[bool] = Field(False, title="Input text")
    """
    Include input text
    """
    input_tokens: Optional[bool] = Field(False, title="Input Tokens")
    """
    Include list of input tokens
    """
    token_logprobs: Optional[bool] = Field(False, title="Token logprobs")
    """
    Include logprob for each returned token
    """
    token_ranks: Optional[bool] = Field(False, title="Token ranks")
    """
    Include rank of each returned token
    """
    top_n_tokens: Optional[int] = Field(None, ge=0, le=5, title="Top N tokens")
    """
    Include top n candidate tokens at the position of each returned token
    """


class TextModeration(ApiBaseModel):
    flagged: bool
    position: ModerationPosition
    score: float
    success: bool
    tokens: Optional[list[ModerationTokens]] = None


class TextTokenizationCreateResults(ApiBaseModel):
    input_text: Optional[str] = None
    token_count: int
    tokens: Optional[list[str]] = None


class TextTokenizationReturnOptions(ApiBaseModel):
    input_text: Optional[bool] = None
    tokens: Optional[bool] = None


class Extensions3(BaseErrorExtension):
    code: Literal["TOO_MANY_REQUESTS"] = "TOO_MANY_REQUESTS"


class TooManyRequestsResponse(BaseErrorResponse):
    extensions: Extensions3
    status_code: Literal[429] = 429


class TrimMethod(str, Enum):
    FLOATING_WINDOW = "floating_window"
    NONE = "none"


class TuneAssetType(str, Enum):
    VECTORS = "vectors"
    LOGS = "logs"
    EXPORT = "export"


class TuneListSortBy(str, Enum):
    STATUS = "status"
    CREATED_AT = "created_at"
    NAME = "name"
    ID = "id"
    MODEL = "model"


class TuneParameters(ApiBaseModel):
    accumulate_steps: Optional[int] = None
    batch_size: Optional[int] = None
    learning_rate: Optional[float] = None
    max_input_tokens: Optional[int] = None
    max_output_tokens: Optional[int] = None
    num_epochs: Optional[int] = None
    num_virtual_tokens: Optional[int] = None
    verbalizer: Optional[str] = None


class TuneResultDatapointLoss(ApiBaseModel):
    epoch: int
    step: Optional[int] = None
    value: float


class TuneResultFiles(ApiBaseModel):
    created_at: Optional[AwareDatetime] = None
    file_name: str
    id: str


class TuneStatus(str, Enum):
    INITIALIZING = "initializing"
    NOT_STARTED = "not_started"
    PENDING = "pending"
    HALTED = "halted"
    RUNNING = "running"
    QUEUED = "queued"
    COMPLETED = "completed"
    FAILED = "failed"


class TunesResultDatapointLoss(ApiBaseModel):
    data: TuneResultDatapointLoss
    timestamp: AwareDatetime


class TuningType(str, Enum):
    PROMPT_TUNING = "prompt_tuning"
    MULTITASK_PROMPT_TUNING = "multitask_prompt_tuning"


class TuningTypeRetrieveResults(ApiBaseModel):
    id: Optional[str] = None
    model_ids: Optional[list[str]] = None
    name: Optional[str] = None
    schema_: Optional[dict[str, Any]] = Field(None, alias="schema")
    """
    JSON Schema
    """


class Extensions4(BaseErrorExtension):
    code: Literal["AUTH_ERROR"] = "AUTH_ERROR"


class UnauthorizedResponse(BaseErrorResponse):
    extensions: Extensions4
    status_code: Literal[401] = 401


class Extensions5(BaseErrorExtension):
    code: Literal["SERVICE_UNAVAILABLE"] = "SERVICE_UNAVAILABLE"


class UnavailableResponse(BaseErrorResponse):
    extensions: Extensions5
    status_code: Literal[503] = 503


class UserCreateResultApiKey(ApiBaseModel):
    created_at: str
    generated_at: str
    last_used_at: Optional[str] = None
    value: str


class _ApiKeyRetrieveParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class ApiKeyRetrieveResponse(ApiBaseModel):
    result: Optional[ApiKeyResult] = None


class _ApiKeyRegenerateCreateParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class ApiKeyRegenerateCreateResponse(ApiBaseModel):
    result: Optional[ApiKeyResult] = None


class _FileRetrieveParametersQuery(ApiBaseModel):
    limit: Optional[int] = Field(100, ge=1, le=100)
    offset: Optional[int] = Field(0, ge=0)
    sort_by: Optional[FileListSortBy] = None
    direction: Optional[SortDirection] = None
    search: Optional[str] = None
    purpose: Optional[FilePurpose] = None
    format_id: Optional[int] = None
    version: Literal["2023-12-15"] = "2023-12-15"


class _FileCreateParametersQuery(ApiBaseModel):
    version: Literal["2023-12-15"] = "2023-12-15"


class _FileCreateRequest(ApiBaseModel):
    file: bytes
    purpose: FilePurpose


class _FileIdDeleteParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _FileIdRetrieveParametersQuery(ApiBaseModel):
    version: Literal["2023-12-15"] = "2023-12-15"


class _FileIdContentRetrieveParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _ModelRetrieveParametersQuery(ApiBaseModel):
    limit: Optional[int] = Field(100, ge=1, le=100)
    offset: Optional[int] = Field(0, ge=0)
    version: Literal["2023-11-22"] = "2023-11-22"


class _ModelIdRetrieveParametersQuery(ApiBaseModel):
    version: Literal["2024-01-30"] = "2024-01-30"


class _PromptRetrieveParametersQuery(ApiBaseModel):
    limit: Optional[int] = Field(100, ge=1, le=100)
    offset: Optional[int] = Field(0, ge=0)
    search: Optional[str] = None
    task_id: Optional[Union[str, list[str]]] = None
    model_id: Optional[Union[str, list[str]]] = None
    source: Optional[Union[PromptRetrieveRequestParamsSource, list[PromptRetrieveRequestParamsSource]]] = None
    version: Literal["2024-01-10"] = "2024-01-10"


class _PromptCreateParametersQuery(ApiBaseModel):
    version: Literal["2024-01-10"] = "2024-01-10"


class _PromptIdDeleteParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _PromptIdRetrieveParametersQuery(ApiBaseModel):
    version: Literal["2024-01-10"] = "2024-01-10"


class _PromptIdPatchParametersQuery(ApiBaseModel):
    version: Literal["2024-01-10"] = "2024-01-10"


class _PromptIdPatchRequest(ApiBaseModel):
    type: Optional[PromptType] = None


class _PromptIdUpdateParametersQuery(ApiBaseModel):
    version: Literal["2024-01-10"] = "2024-01-10"


class _RequestRetrieveParametersQuery(ApiBaseModel):
    limit: Optional[int] = Field(100, ge=1, le=100)
    offset: Optional[int] = Field(0, ge=0)
    status: Optional[RequestStatus] = None
    origin: Optional[RequestOrigin] = None
    before: Optional[AwareDatetime] = None
    after: Optional[AwareDatetime] = None
    endpoint: Optional[Union[RequestEndpoint, list[RequestEndpoint]]] = None
    api: Optional[RequestApiVersion] = None
    date_: Optional[date] = Field(None, alias="date")
    version: Literal["2023-11-22"] = "2023-11-22"


class _RequestChatConversationIdDeleteParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _RequestChatConversationIdRetrieveParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _RequestIdDeleteParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _SystemPromptRetrieveParametersQuery(ApiBaseModel):
    limit: Optional[int] = Field(100, ge=1, le=100)
    offset: Optional[int] = Field(0, ge=0)
    version: Literal["2023-11-22"] = "2023-11-22"


class _SystemPromptCreateParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _SystemPromptCreateRequest(ApiBaseModel):
    content: str
    name: str


class _SystemPromptIdDeleteParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _SystemPromptIdRetrieveParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _SystemPromptIdUpdateParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _SystemPromptIdUpdateRequest(ApiBaseModel):
    content: str
    name: str


class _TaskRetrieveParametersQuery(ApiBaseModel):
    tune: Optional[bool] = True
    version: Literal["2023-11-22"] = "2023-11-22"


class TaskRetrieveResponse(ApiBaseModel):
    results: list[Tasks]


class _TextChatCreateParametersQuery(ApiBaseModel):
    version: Literal["2024-01-10"] = "2024-01-10"


class _TextChatOutputCreateParametersQuery(ApiBaseModel):
    version: Literal["2024-01-10"] = "2024-01-10"


class TextChatOutputCreateResponse(ApiBaseModel):
    result: str


class _TextChatStreamCreateParametersQuery(ApiBaseModel):
    version: Literal["2024-01-10"] = "2024-01-10"


class _TextEmbeddingCreateParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class Input(RootModel[list[Any]]):
    root: list[Any] = Field(..., max_length=20)


class _TextEmbeddingCreateRequest(ApiBaseModel):
    input: Union[str, Input]
    model_id: str
    parameters: Optional[TextEmbeddingParameters] = None


class TextEmbeddingCreateResponse(ApiBaseModel):
    results: list[list[float]]


class _TextEmbeddingLimitRetrieveParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class TextEmbeddingLimitRetrieveResponse(ApiBaseModel):
    result: TextEmbeddingLimit


class _TextExtractionLimitRetrieveParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _TextGenerationCreateParametersQuery(ApiBaseModel):
    version: Literal["2024-01-10"] = "2024-01-10"


class _TextGenerationComparisonCreateParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _TextGenerationLimitRetrieveParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class TextGenerationLimitRetrieveResponse(ApiBaseModel):
    result: TextGenerationLimit


class _TextGenerationOutputCreateParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class TextGenerationOutputCreateResponse(ApiBaseModel):
    results: list[str]


class _TextGenerationIdFeedbackRetrieveParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class TextGenerationIdFeedbackRetrieveResponse(ApiBaseModel):
    result: TextGenerationFeedbackResult


class _TextGenerationIdFeedbackCreateParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _TextGenerationIdFeedbackCreateRequest(ApiBaseModel):
    categories: Optional[list[TextGenerationFeedbackCategory]] = Field(None, min_length=1)
    comment: Optional[str] = None


class TextGenerationIdFeedbackCreateResponse(ApiBaseModel):
    result: TextGenerationFeedbackResult


class _TextGenerationIdFeedbackUpdateParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _TextGenerationIdFeedbackUpdateRequest(ApiBaseModel):
    categories: Optional[list[TextGenerationFeedbackCategory]] = Field(None, min_length=1)
    comment: Optional[str] = None


class TextGenerationIdFeedbackUpdateResponse(ApiBaseModel):
    result: TextGenerationFeedbackResult


class _TextGenerationStreamCreateParametersQuery(ApiBaseModel):
    version: Literal["2024-01-10"] = "2024-01-10"


class _TextModerationCreateParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _TextModerationCreateRequest(ApiBaseModel):
    hap: Optional[HAPOptions] = None
    implicit_hate: Optional[ImplicitHateOptions] = None
    input: str = Field(..., max_length=20480)
    stigma: Optional[StigmaOptions] = None


class _TextTokenizationCreateParametersQuery(ApiBaseModel):
    version: Literal["2024-01-10"] = "2024-01-10"


class TextTokenizationCreateResponse(ApiBaseModel):
    created_at: str
    model_id: str
    results: list[TextTokenizationCreateResults]


class _TuneRetrieveParametersQuery(ApiBaseModel):
    limit: Optional[int] = Field(100, ge=1, le=100)
    offset: Optional[int] = Field(0, ge=0)
    status: Optional[TuneStatus] = None
    search: Optional[str] = None
    sort_by: Optional[TuneListSortBy] = None
    direction: Optional[SortDirection] = None
    version: Literal["2023-11-22"] = "2023-11-22"


class _TuneCreateParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _TuneCreateRequest(ApiBaseModel):
    evaluation_file_ids: Optional[list[str]] = None
    model_id: str
    name: str
    parameters: Optional[TuneParameters] = None
    task_id: str
    training_file_ids: list[str]
    tuning_type: TuningType
    validation_file_ids: Optional[list[str]] = None


class _TuneImportCreateParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _TuneImportCreateRequest(ApiBaseModel):
    file_id: str
    name: str


class _TuneIdDeleteParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _TuneIdRetrieveParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _TuneIdPatchParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _TuneIdPatchRequest(ApiBaseModel):
    name: Optional[str] = None
    preferred: Optional[bool] = None


class _TuneIdContentTypeRetrieveParametersQuery(ApiBaseModel):
    version: Literal["2023-12-15"] = "2023-12-15"


class _TuningTypeRetrieveParametersQuery(ApiBaseModel):
    version: Literal["2024-01-30"] = "2024-01-30"


class TuningTypeRetrieveResponse(ApiBaseModel):
    results: list[TuningTypeRetrieveResults]


class _UserDeleteParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _UserRetrieveParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _UserPatchParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _UserPatchRequest(ApiBaseModel):
    data_usage_consent: Optional[bool] = None
    tou_accepted: Optional[bool] = None


class _UserCreateParametersQuery(ApiBaseModel):
    version: Literal["2023-11-22"] = "2023-11-22"


class _UserCreateRequest(ApiBaseModel):
    first_name: Optional[str] = None
    last_name: Optional[str] = None


class Extensions(BaseErrorExtension):
    code: Literal["INVALID_INPUT"] = "INVALID_INPUT"


class BadRequestResponse(BaseErrorResponse):
    extensions: Extensions
    status_code: Literal[400] = 400


class BaseMessage(ApiBaseModel):
    content: str
    file_ids: Optional[list[str]] = None
    role: ChatRole


class BaseTokens(ApiBaseModel):
    logprob: Optional[Union[float, str]] = None
    rank: Optional[int] = None
    text: Optional[str] = None
    top_tokens: Optional[list[GeneratedToken]] = None


class FileResult(ApiBaseModel):
    bytes: int
    created_at: AwareDatetime
    file_formats: Optional[list[FileFormat]] = None
    file_name: str
    id: str
    purpose: FilePurpose
    storage_provider_location: StorageProviderLocation


class ModelIdRetrieveResult(ApiBaseModel):
    description: Optional[str] = None
    developer: Optional[str] = None
    disabled: bool
    id: str
    is_live: bool
    label: str
    model_family: ModelFamily
    name: str
    preferred: bool
    size: str
    source_model_id: Optional[str] = None
    system_prompt: Optional[str] = None
    system_prompt_id: Optional[int] = None
    tags: list[str]
    tasks: list[Tasks]
    token_limits: list[ModelTokenLimits]
    warning: Optional[str] = None


class ModelRetrieveResults(ApiBaseModel):
    id: str
    is_live: bool
    label: str
    name: str
    size: str
    source_model_id: Optional[str] = None
    task_ids: list[str]
    token_limits: list[ModelTokenLimits]
    warning: Optional[str] = None


class ModerationParameters(ApiBaseModel):
    hap: Optional[Union[bool, ModerationHAP]] = None
    implicit_hate: Optional[Union[bool, ModerationImplicitHate]] = None
    stigma: Optional[Union[bool, ModerationStigma]] = None


class PromptTemplate(ApiBaseModel):
    data: PromptTemplateData
    id: Optional[str] = None
    value: Optional[str] = None


class RequestRetrieveResults(ApiBaseModel):
    created_at: AwareDatetime
    duration: int
    id: str
    request: Optional[dict[str, Any]] = None
    response: Optional[dict[str, Any]] = None
    status: RequestStatus
    version: Optional[RequestResultVersion] = None


class SystemPrompt(ApiBaseModel):
    author: Optional[SystemPromptAuthor] = None
    content: str
    created_at: AwareDatetime
    id: int
    name: str
    type: SystemPromptType


class TextCreateResponseModeration(ApiBaseModel):
    hap: Optional[list[TextModeration]] = None
    implicit_hate: Optional[list[TextModeration]] = None
    stigma: Optional[list[TextModeration]] = None


class TextGenerationComparisonParameters(ApiBaseModel):
    length_penalty: Optional[list[dict[str, Any]]] = Field(None, max_length=10, min_length=1)
    model_id: Optional[list[str]] = Field(None, max_length=10, min_length=1)
    repetition_penalty: Optional[list[float]] = Field(None, max_length=10, min_length=1)
    temperature: Optional[list[float]] = Field(None, max_length=10, min_length=1)
    top_k: Optional[list[int]] = Field(None, max_length=10, min_length=1)
    top_p: Optional[list[float]] = Field(None, max_length=10, min_length=1)
    typical_p: Optional[list[float]] = Field(None, max_length=10, min_length=1)


class TextGenerationParameters(ApiBaseModel):
    beam_width: Optional[int] = Field(None, ge=0, le=3, title="Beam width")
    """
    At each step, or token, the algorithm keeps track of the n (off=1, 2, or 3) most probable sequences (beams) and selects the one with the highest probability. This continues until the stop sequence is met.
    """
    decoding_method: Optional[DecodingMethod] = None
    include_stop_sequence: Optional[bool] = None
    length_penalty: Optional[LengthPenalty] = None
    max_new_tokens: Optional[int] = Field(None, ge=0, le=4096, title="Max new tokens")
    """
    Define the maximum number of tokens to generate.
    """
    min_new_tokens: Optional[int] = Field(None, ge=0, title="Min new tokens")
    """
    If stop sequences are given, they are ignored until minimum tokens are generated.
    """
    random_seed: Optional[int] = Field(None, ge=1, le=4294967295, title="Random seed")
    """
    Controls the random sampling of the generated tokens when sampling is enabled. Setting the random seed to a the same number for each generation ensures experimental repeatability.
    """
    repetition_penalty: Optional[float] = Field(None, ge=1.0, le=2.0, multiple_of=0.01, title="Repetition penalty")
    """
    The parameter for repetition penalty. 1.00 means no penalty.
    """
    return_options: Optional[TextGenerationReturnOptions] = None
    stop_sequences: Optional[list[str]] = Field(
        None, examples=['[" and "]'], max_length=6, min_length=1, title="Stop sequences"
    )
    """
    Stop sequences are one or more strings which will cause the text generation to stop if/when they are produced as part of the output. Stop sequences encountered prior to the minimum number of tokens being generated will be ignored.
    """
    temperature: Optional[float] = Field(None, ge=0.0, le=2.0, multiple_of=0.01, title="Temperature")
    """
    Control the creativity of generated text. Higher values will lead to more randomly generated outputs.
    """
    time_limit: Optional[int] = Field(None, title="Time limit")
    """
    Time limit in milliseconds - if not completed within this time, generation will stop. The text generated so far will be returned along with the `TIME_LIMIT` stop reason.
    """
    top_k: Optional[int] = Field(None, ge=1, le=100, title="Top K")
    """
    Set the number of highest probability vocabulary tokens to keep for top-k-filtering. Lower values make it less likely the model will go off topic.
    """
    top_p: Optional[float] = Field(None, ge=0.0, le=1.0, multiple_of=0.01, title="Top P (nucleus sampling)")
    """
    If < 1.0, only the smallest set of most probable tokens with probabilities that add up to `top_p` or higher are used.
    """
    truncate_input_tokens: Optional[int] = Field(None, ge=0, title="Truncate input tokens")
    """
    Truncate to this many input tokens. Can be used to avoid requests failing due to input being longer than configured limits. Zero means don't truncate.
    """
    typical_p: Optional[float] = Field(None, ge=0.01, le=1.0, multiple_of=0.01, title="Typical P")
    """
    Local typicality measures how similar the conditional probability of predicting a target token next is to the expected conditional probability of predicting a random token next, given the partial text already generated. If set to float < 1, the smallest set of the most locally typical tokens with probabilities that add up to typical_p or higher are kept for generation. 1.00 means a neutral value.
    """


class TextGenerationResult(ApiBaseModel):
    generated_text: str
    generated_token_count: int
    generated_tokens: Optional[list[BaseTokens]] = None
    input_text: Optional[str] = None
    input_token_count: Optional[int] = None
    input_tokens: Optional[list[BaseTokens]] = None
    moderation: Optional[dict[str, Any]] = None
    seed: Optional[float] = None
    stop_reason: StopReason
    stop_sequence: Optional[str] = None


class TextGenerationStreamResult(ApiBaseModel):
    generated_text: str
    generated_token_count: int
    generated_tokens: Optional[list[BaseTokens]] = None
    input_text: Optional[str] = None
    input_token_count: Optional[int] = None
    input_tokens: Optional[list[BaseTokens]] = None
    seed: Optional[float] = None
    stop_reason: StopReason
    stop_sequence: Optional[str] = None


class TextTokenizationParameters(ApiBaseModel):
    return_options: Optional[TextTokenizationReturnOptions] = None


class TuneResultDatapoints(ApiBaseModel):
    loss: list[TunesResultDatapointLoss]


class UserGenerationDefault(ApiBaseModel):
    model_id: Optional[str] = None
    moderations: Optional[ModerationParameters] = None
    parameters: Optional[TextGenerationParameters] = None
    prompt_id: Optional[str] = None
    template: Optional[PromptTemplate] = None


class UserResponseResult(ApiBaseModel):
    data_usage_consent: bool
    first_name: Optional[str] = None
    generate_default: Optional[UserGenerationDefault] = None
    id: int
    last_name: Optional[str] = None
    tou_accepted: bool
    tou_accepted_at: Optional[str] = None


class FileRetrieveResponse(ApiBaseModel):
    results: list[FileResult]
    total_count: int


class FileCreateResponse(ApiBaseModel):
    result: FileResult


class FileIdRetrieveResponse(ApiBaseModel):
    result: FileResult


class ModelRetrieveResponse(ApiBaseModel):
    results: list[ModelRetrieveResults]


class ModelIdRetrieveResponse(ApiBaseModel):
    result: ModelIdRetrieveResult


class _PromptCreateRequest(ApiBaseModel):
    data: Optional[PromptTemplateData] = None
    description: Optional[str] = None
    input: Optional[str] = None
    messages: Optional[list[BaseMessage]] = None
    model_id: str
    moderations: Optional[ModerationParameters] = None
    name: str
    output: Optional[str] = Field(None, min_length=1)
    parameters: Optional[TextGenerationParameters] = None
    prompt_id: Optional[str] = None
    task_id: Optional[str] = None
    type: Optional[PromptType] = None


class _PromptIdUpdateRequest(ApiBaseModel):
    data: Optional[PromptTemplateData] = None
    description: Optional[str] = None
    input: Optional[str] = None
    messages: Optional[list[BaseMessage]] = None
    model_id: str
    moderations: Optional[ModerationParameters] = None
    name: str
    output: Optional[str] = Field(None, min_length=1)
    parameters: Optional[TextGenerationParameters] = None
    prompt_id: Optional[str] = None
    task_id: Optional[str] = None
    type: Optional[PromptType] = None


class RequestRetrieveResponse(ApiBaseModel):
    results: list[RequestRetrieveResults]
    total_count: int


class SystemPromptRetrieveResponse(ApiBaseModel):
    results: list[SystemPrompt]
    total_count: int


class SystemPromptCreateResponse(ApiBaseModel):
    result: SystemPrompt


class SystemPromptIdRetrieveResponse(ApiBaseModel):
    result: SystemPrompt


class SystemPromptIdUpdateResponse(ApiBaseModel):
    result: SystemPrompt


class _TextChatCreateRequest(ApiBaseModel):
    conversation_id: Optional[str] = None
    messages: Optional[list[BaseMessage]] = Field(None, min_length=1)
    model_id: Optional[str] = None
    moderations: Optional[ModerationParameters] = None
    parameters: Optional[TextGenerationParameters] = None
    parent_id: Optional[str] = None
    prompt_id: Optional[str] = None
    prompt_template_id: Optional[str] = None
    trim_method: Optional[TrimMethod] = None
    use_conversation_parameters: Optional[bool] = None


class TextChatCreateResponse(ApiBaseModel):
    conversation_id: str
    created_at: Optional[AwareDatetime] = None
    id: Optional[str] = None
    input_parameters: Optional[dict[str, Any]] = None
    model_id: Optional[str] = None
    results: list[TextGenerationResult]


class _TextChatOutputCreateRequest(ApiBaseModel):
    conversation_id: Optional[str] = None
    messages: Optional[list[BaseMessage]] = Field(None, min_length=1)
    model_id: Optional[str] = None
    moderations: Optional[ModerationParameters] = None
    parameters: Optional[TextGenerationParameters] = None
    parent_id: Optional[str] = None
    prompt_id: Optional[str] = None
    prompt_template_id: Optional[str] = None
    trim_method: Optional[TrimMethod] = None
    use_conversation_parameters: Optional[bool] = None


class _TextChatStreamCreateRequest(ApiBaseModel):
    conversation_id: Optional[str] = None
    messages: Optional[list[BaseMessage]] = Field(None, min_length=1)
    model_id: Optional[str] = None
    moderations: Optional[ModerationParameters] = None
    parameters: Optional[TextGenerationParameters] = None
    parent_id: Optional[str] = None
    prompt_id: Optional[str] = None
    prompt_template_id: Optional[str] = None
    trim_method: Optional[TrimMethod] = None
    use_conversation_parameters: Optional[bool] = None


class TextChatStreamCreateResponse(ApiBaseModel):
    conversation_id: str
    created_at: Optional[AwareDatetime] = None
    id: Optional[str] = None
    input_parameters: Optional[dict[str, Any]] = None
    model_id: Optional[str] = None
    moderation: Optional[TextCreateResponseModeration] = None
    results: Optional[list[TextGenerationStreamResult]] = None


class _TextGenerationCreateRequest(ApiBaseModel):
    data: Optional[PromptTemplateData] = None
    input: Optional[str] = Field(None, examples=["How are you"], title="Input string")
    """
    The input is the prompt to generate completions, passed as a string. Note: The method tokenizes the input internally. It is recommended not to leave any trailing spaces.
    """
    model_id: Optional[str] = Field(None, title="Model ID")
    """
    The ID of the model or tune to be used for this request.
    """
    moderations: Optional[ModerationParameters] = None
    parameters: Optional[TextGenerationParameters] = None
    prompt_id: Optional[str] = Field(None, min_length=1, title="Saved prompt Id")


class TextGenerationCreateResponse(ApiBaseModel):
    created_at: AwareDatetime
    id: str
    input_parameters: Optional[dict[str, Any]] = None
    model_id: str
    results: list[TextGenerationResult]


class _TextGenerationOutputCreateRequest(ApiBaseModel):
    data: Optional[PromptTemplateData] = None
    input: Optional[str] = None
    model_id: Optional[str] = None
    moderations: Optional[ModerationParameters] = None
    parameters: Optional[TextGenerationParameters] = None
    prompt_id: Optional[str] = None
    use_default: Optional[bool] = None


class _TextGenerationStreamCreateRequest(ApiBaseModel):
    data: Optional[PromptTemplateData] = None
    input: Optional[str] = Field(None, examples=["How are you"], title="Input string")
    """
    The input is the prompt to generate completions, passed as a string. Note: The method tokenizes the input internally. It is recommended not to leave any trailing spaces.
    """
    model_id: Optional[str] = Field(None, title="Model ID")
    """
    The ID of the model or tune to be used for this request.
    """
    moderations: Optional[ModerationParameters] = None
    parameters: Optional[TextGenerationParameters] = None
    prompt_id: Optional[str] = Field(None, min_length=1, title="Saved prompt Id")


class TextGenerationStreamCreateResponse(ApiBaseModel):
    created_at: Optional[AwareDatetime] = None
    id: Optional[str] = None
    input_parameters: Optional[dict[str, Any]] = None
    model_id: str
    moderation: Optional[TextCreateResponseModeration] = None
    results: Optional[list[TextGenerationStreamResult]] = None


class TextModerationCreateResponse(ApiBaseModel):
    results: list[TextCreateResponseModeration]


class _TextTokenizationCreateRequest(ApiBaseModel):
    data: Optional[PromptTemplateData] = None
    input: Optional[Union[str, list[str]]] = None
    model_id: Optional[str] = None
    parameters: Optional[TextTokenizationParameters] = None
    prompt_id: Optional[str] = None


class UserRetrieveResponse(ApiBaseModel):
    result: UserResponseResult


class UserPatchResponse(ApiBaseModel):
    result: UserResponseResult


class PromptsResponseResult(ApiBaseModel):
    author: Optional[PromptResultAuthor] = None
    created_at: AwareDatetime
    data: Optional[dict[str, Any]] = None
    description: Optional[str] = None
    id: str
    input: Optional[str] = None
    messages: Optional[list[BaseMessage]] = None
    metadata: Optional[dict[str, Any]] = None
    model_id: Optional[str] = None
    moderations: Optional[ModerationParameters] = None
    name: str
    output: Optional[str] = None
    parameters: Optional[TextGenerationParameters] = None
    prompt_id: Optional[str] = None
    public: Optional[bool] = None
    task: Optional[PromptResultTask] = None
    type: PromptType


class RequestChatConversationIdRetrieveResultsRequest(ApiBaseModel):
    conversation_id: Optional[str] = None
    messages: Optional[list[BaseMessage]] = Field(None, min_length=1)
    model_id: Optional[str] = None
    moderations: Optional[ModerationParameters] = None
    parameters: Optional[TextGenerationParameters] = None
    parent_id: Optional[str] = None
    prompt_id: Optional[str] = None
    prompt_template_id: Optional[str] = None
    trim_method: Optional[TrimMethod] = None
    use_conversation_parameters: Optional[bool] = None


class RequestChatConversationIdRetrieveResultsResponse(ApiBaseModel):
    conversation_id: str
    created_at: Optional[AwareDatetime] = None
    id: Optional[str] = None
    input_parameters: Optional[dict[str, Any]] = None
    model_id: Optional[str] = None
    results: list[TextGenerationResult]


class TextGenerationComparisonCreateRequestRequest(ApiBaseModel):
    data: Optional[PromptTemplateData] = None
    input: str
    model_id: Optional[str] = None
    moderations: Optional[ModerationParameters] = None
    parameters: Optional[TextGenerationParameters] = None
    prompt_id: Optional[str] = None
    use_default: Optional[bool] = None


class TextGenerationComparisonCreateResultsResult(ApiBaseModel):
    created_at: AwareDatetime
    id: str
    input_parameters: Optional[dict[str, Any]] = None
    model_id: str
    results: list[TextGenerationResult]


class TuneResult(ApiBaseModel):
    created_at: AwareDatetime
    datapoints: Optional[TuneResultDatapoints] = None
    evaluation_files: Optional[list[TuneResultFiles]] = None
    finished_at: Optional[AwareDatetime] = None
    id: str
    model_id: str
    model_name: str
    name: str
    parameters: Optional[dict[str, Any]] = None
    preferred: bool
    started_at: Optional[AwareDatetime] = None
    status: TuneStatus
    status_message: Optional[str] = None
    task_id: str
    task_name: str
    training_files: Optional[list[TuneResultFiles]] = None
    tuning_type: str
    validation_files: Optional[list[TuneResultFiles]] = None
    vectors: Optional[str] = None


class UserCreateResult(ApiBaseModel):
    api_key: UserCreateResultApiKey
    data_usage_consent: bool
    first_name: Optional[str] = None
    generate_default: Optional[UserGenerationDefault] = None
    id: int
    last_name: Optional[str] = None
    tou_accepted: bool
    tou_accepted_at: Optional[str] = None
    user_id: str


class PromptRetrieveResponse(ApiBaseModel):
    results: list[PromptsResponseResult]
    total_count: int


class PromptCreateResponse(ApiBaseModel):
    result: PromptsResponseResult


class PromptIdRetrieveResponse(ApiBaseModel):
    result: PromptsResponseResult


class PromptIdPatchResponse(ApiBaseModel):
    result: PromptsResponseResult


class PromptIdUpdateResponse(ApiBaseModel):
    result: PromptsResponseResult


class _TextGenerationComparisonCreateRequest(ApiBaseModel):
    compare_parameters: TextGenerationComparisonParameters
    name: Optional[str] = None
    request: TextGenerationComparisonCreateRequestRequest


class TuneRetrieveResponse(ApiBaseModel):
    results: list[TuneResult]
    total_count: int


class TuneCreateResponse(ApiBaseModel):
    result: TuneResult


class TuneImportCreateResponse(ApiBaseModel):
    result: TuneResult


class TuneIdRetrieveResponse(ApiBaseModel):
    result: TuneResult


class TuneIdPatchResponse(ApiBaseModel):
    result: TuneResult


class UserCreateResponse(ApiBaseModel):
    result: UserCreateResult


class RequestChatConversationIdRetrieveResults(ApiBaseModel):
    created_at: AwareDatetime
    duration: int
    id: str
    parent_id: Optional[str] = None
    request: RequestChatConversationIdRetrieveResultsRequest
    response: RequestChatConversationIdRetrieveResultsResponse
    status: RequestStatus
    version: Optional[RequestResultVersion] = None


class TextGenerationComparisonCreateResults(ApiBaseModel):
    error: Optional[Any] = None
    parameters: TextGenerationComparisonCreateResultsParameters
    result: Optional[TextGenerationComparisonCreateResultsResult] = None


class RequestChatConversationIdRetrieveResponse(ApiBaseModel):
    results: list[RequestChatConversationIdRetrieveResults]


class TextGenerationComparisonCreateResponse(ApiBaseModel):
    results: list[TextGenerationComparisonCreateResults]


__all__ = [
    "ApiKeyRegenerateCreateResponse",
    "ApiKeyResult",
    "ApiKeyRetrieveResponse",
    "BadRequestResponse",
    "BaseErrorExtension",
    "BaseErrorResponse",
    "BaseMessage",
    "BaseTokens",
    "ChatRole",
    "ConcurrencyLimit",
    "DecodingMethod",
    "Extensions",
    "Extensions1",
    "Extensions2",
    "Extensions3",
    "Extensions4",
    "Extensions5",
    "FileCreateResponse",
    "FileFormat",
    "FileIdRetrieveResponse",
    "FileListSortBy",
    "FilePurpose",
    "FileResult",
    "FileRetrieveResponse",
    "GeneratedToken",
    "HAPOptions",
    "ImplicitHateOptions",
    "Input",
    "InternalServerErrorResponse",
    "LengthPenalty",
    "ModelFamily",
    "ModelIdRetrieveResponse",
    "ModelIdRetrieveResult",
    "ModelRetrieveResponse",
    "ModelRetrieveResults",
    "ModelTokenLimits",
    "ModerationHAP",
    "ModerationImplicitHate",
    "ModerationParameters",
    "ModerationPosition",
    "ModerationStigma",
    "ModerationTokens",
    "NotFoundResponse",
    "PromptCreateResponse",
    "PromptIdPatchResponse",
    "PromptIdRetrieveResponse",
    "PromptIdUpdateResponse",
    "PromptResultAuthor",
    "PromptResultTask",
    "PromptRetrieveRequestParamsSource",
    "PromptRetrieveResponse",
    "PromptTemplate",
    "PromptTemplateData",
    "PromptType",
    "PromptsResponseResult",
    "RequestApiVersion",
    "RequestChatConversationIdRetrieveResponse",
    "RequestChatConversationIdRetrieveResults",
    "RequestChatConversationIdRetrieveResultsRequest",
    "RequestChatConversationIdRetrieveResultsResponse",
    "RequestEndpoint",
    "RequestOrigin",
    "RequestResultVersion",
    "RequestRetrieveResponse",
    "RequestRetrieveResults",
    "RequestStatus",
    "SortDirection",
    "StigmaOptions",
    "StopReason",
    "StorageProviderLocation",
    "SystemPrompt",
    "SystemPromptAuthor",
    "SystemPromptCreateResponse",
    "SystemPromptIdRetrieveResponse",
    "SystemPromptIdUpdateResponse",
    "SystemPromptRetrieveResponse",
    "SystemPromptType",
    "TaskRetrieveResponse",
    "Tasks",
    "TextChatCreateResponse",
    "TextChatOutputCreateResponse",
    "TextChatStreamCreateResponse",
    "TextCreateResponseModeration",
    "TextEmbeddingCreateResponse",
    "TextEmbeddingLimit",
    "TextEmbeddingLimitRetrieveResponse",
    "TextEmbeddingParameters",
    "TextGenerationComparisonCreateRequestRequest",
    "TextGenerationComparisonCreateResponse",
    "TextGenerationComparisonCreateResults",
    "TextGenerationComparisonCreateResultsParameters",
    "TextGenerationComparisonCreateResultsResult",
    "TextGenerationComparisonParameters",
    "TextGenerationCreateResponse",
    "TextGenerationFeedbackCategory",
    "TextGenerationFeedbackResult",
    "TextGenerationIdFeedbackCreateResponse",
    "TextGenerationIdFeedbackRetrieveResponse",
    "TextGenerationIdFeedbackUpdateResponse",
    "TextGenerationLimit",
    "TextGenerationLimitRetrieveResponse",
    "TextGenerationOutputCreateResponse",
    "TextGenerationParameters",
    "TextGenerationResult",
    "TextGenerationReturnOptions",
    "TextGenerationStreamCreateResponse",
    "TextGenerationStreamResult",
    "TextModeration",
    "TextModerationCreateResponse",
    "TextTokenizationCreateResponse",
    "TextTokenizationCreateResults",
    "TextTokenizationParameters",
    "TextTokenizationReturnOptions",
    "TooManyRequestsResponse",
    "TrimMethod",
    "TuneAssetType",
    "TuneCreateResponse",
    "TuneIdPatchResponse",
    "TuneIdRetrieveResponse",
    "TuneImportCreateResponse",
    "TuneListSortBy",
    "TuneParameters",
    "TuneResult",
    "TuneResultDatapointLoss",
    "TuneResultDatapoints",
    "TuneResultFiles",
    "TuneRetrieveResponse",
    "TuneStatus",
    "TunesResultDatapointLoss",
    "TuningType",
    "TuningTypeRetrieveResponse",
    "TuningTypeRetrieveResults",
    "UnauthorizedResponse",
    "UnavailableResponse",
    "UserCreateResponse",
    "UserCreateResult",
    "UserCreateResultApiKey",
    "UserGenerationDefault",
    "UserPatchResponse",
    "UserResponseResult",
    "UserRetrieveResponse",
]
